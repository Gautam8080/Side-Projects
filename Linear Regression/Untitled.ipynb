{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "\n",
    "# A class that models the Neural Net with L-layers and\n",
    "# N neurons in each layer. It also contains the functions\n",
    "# for training, testing, and optimizing the Neural Network\n",
    "class DeepNN:\n",
    "\n",
    "    # Constructor to build the structure of the Neural Network\n",
    "    # It accepts the layers in the format of [2,3,1] -> 2 Neuron Input Layer,\n",
    "    # 3 Neuron Hidden Layer and 1 Neuron output layer\n",
    "    def __init__(self, layers):\n",
    "        ############################### Initialize the number of layers and neurons\n",
    "        self.layers = layers\n",
    "        self.num_layers = len(layers)\n",
    "        self.hidden_layers = len(layers) - 2\n",
    "        self.input_neurons = layers[0]\n",
    "        self.output_neurons = layers[-1]\n",
    "        \n",
    "        ########## Intialize parameters for Forward Propogation\n",
    "        # Initialize Weights\n",
    "        self.epsilon = 0.12  # Random Weight Initialization Factor\n",
    "        self.weights = []\n",
    "        for i in range(self.num_layers-2):\n",
    "            self.weights.append(np.random.randn(layers[i]+1, layers[i+1]+1)*2*self.epsilon - self.epsilon)\n",
    "                        # We add a +1 to incorporate for weights from the +1 neuron for the bias\n",
    "        self.weights.append(np.random.randn(layers[-2]+1, layers[-1])*2*self.epsilon - self.epsilon)\n",
    "        \n",
    "        self.a = [] # To keep track of activations\n",
    "        self.z = [] # To keep track of layer values\n",
    "        self.activations = ['sigmoid']*(self.num_layers-1) # Activations for each layer\n",
    "        \n",
    "        ######### Intialize parameters for Backward Propogation\n",
    "        self.delta = []\n",
    "        self.gradient = []\n",
    "        \n",
    "    ################################### Define Some Activation Functions and their derivatives ##################\n",
    "    def sigmoid(self,z):\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "    def sigmoidPrime(self,z):\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "\n",
    "    def reLU(self,x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def reLUPrime(self,x):\n",
    "        return np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "    def softmax(self,x):\n",
    "        return np.exp(x)/np.sum(np.exp(x), axis = 0)\n",
    "\n",
    "    def tanh(self,z):\n",
    "        return np.tanh(z)\n",
    "\n",
    "    def tanh_prime(self,x):\n",
    "        return 1 - np.tanh(x)**2\n",
    "    \n",
    "    def identity(self,x):\n",
    "        return x\n",
    "    \n",
    "    def identity_prime(self,x):\n",
    "        return 1\n",
    "\n",
    "    ######################################### Cost Functions #############################################\n",
    "    # Least Squares\n",
    "    def least_squares_cost(self,t):\n",
    "        return 0.5*np.mean( (t-self.a[-1])**2 )\n",
    "\n",
    "    # Cross Entropy Log Loss Function\n",
    "    def log_loss(self,t):\n",
    "        return np.mean( ( -1*t*np.log(self.a[-1]) - (1-t)*np.log(1-self.a[-1]) ) )\n",
    "\n",
    "    ######################################### Forward Feed ##############################################\n",
    "    def forwardFeed(self, X, activations):\n",
    "        self.activations = activations\n",
    "        a = [X] # Keep Track of activations\n",
    "        z = []\n",
    "        \n",
    "        # Add Bias\n",
    "        c = np.ones([1,a[0].shape[0]]).reshape(a[0].shape[0],1)\n",
    "        a[0] = np.concatenate((c,a[0]), axis=1)\n",
    "#         print(a)\n",
    "        for i in range(self.num_layers-1):\n",
    "#             print(a[i])\n",
    "            z.append(np.dot(a[i],self.weights[i]))\n",
    "            if(activations[i] == 'sigmoid'):\n",
    "                a.append(self.sigmoid(z[i]))\n",
    "            elif(activations[i] == 'reLU'):\n",
    "                a.append(self.reLU(z[i]))\n",
    "            elif(activations[i] == 'tanh'):\n",
    "                a.append(self.tanh(z[i]))\n",
    "            elif(activations[i] == 'softmax'):\n",
    "                a.append(self.softmax(z[i]))\n",
    "            elif(activations[i] == 'identity'):\n",
    "                a.append(self.identity(z[i]))\n",
    "        self.a = a\n",
    "        self.z = z\n",
    "    \n",
    "    def backPropogate(self,y):\n",
    "        \n",
    "        delta = []\n",
    "        gradient = []\n",
    "#         print('Weights:', self.weights)\n",
    "        weights_flipped = self.weights[::-1]\n",
    "        z_flipped = self.z[::-1]\n",
    "        activations_flipped = self.a[::-1]\n",
    "        activation_func_flipped = self.activations[::-1]\n",
    "        delta.append(activations_flipped[0] - y)\n",
    "#         print('Weights Flipped:', weights_flipped)\n",
    "        for i in range(0,self.num_layers-2):\n",
    "#                 print('delta: ',delta[i])\n",
    "#                 print('weights_flipped: ',weights_flipped[i])\n",
    "#                 print('z_flipped: ',z_flipped[i+1])\n",
    "                if(activation_func_flipped[i] == 'sigmoid'):\n",
    "#                     print('Sigmoid Prime')\n",
    "                    delta.append( np.dot(delta[i], weights_flipped[i].T ) * self.sigmoidPrime(z_flipped[i+1]) )\n",
    "                elif(activation_func_flipped[i] == 'reLU'):\n",
    "#                     print('reLU Prime')\n",
    "                    delta.append( np.dot(delta[i], weights_flipped[i].T ) * self.reLUPrime(z_flipped[i+1]) )\n",
    "                elif(activation_func_flipped[i] == 'tanh'):\n",
    "                    delta.append( np.dot(delta[i], weights_flipped[i].T ) * self.tanh_prime(z_flipped[i+1]) )\n",
    "                elif(activation_func_flipped[i] == 'identity'):\n",
    "                    delta.append( np.dot(delta[i], weights_flipped[i].T ) * self.identity_prime(z_flipped[i+1]) )\n",
    "                elif(activation_func_flipped[i] == 'softmax'):\n",
    "                    delta.append( np.dot(delta[i], weights_flipped[i].T))\n",
    "                                 \n",
    "        delta = delta[::-1]\n",
    "    \n",
    "        for i in range(len(delta)):\n",
    "            gradient.append( np.dot(self.a[i].T, delta[i]) )\n",
    "            \n",
    "        self.delta = delta  \n",
    "        self.gradient = gradient\n",
    "        \n",
    "    def learn(self, num_of_epochs, learning_rate, X, y, activations,cost_func):\n",
    "        for i in range(num_of_epochs):\n",
    "                self.forwardFeed(X, activations)\n",
    "                self.backPropogate(y)\n",
    "                \n",
    "                for j in range(len(self.gradient)):\n",
    "                    self.weights[j] = self.weights[j] - learning_rate*self.gradient[j]\n",
    "                    \n",
    "                \n",
    "                if(num_of_epochs > 10000):\n",
    "                    if(i%10000 == 0):\n",
    "                        if(cost_func == 'log_loss'):\n",
    "                            cost = self.log_loss(y)\n",
    "                            print('Cost: ', cost)\n",
    "                        elif(cost_func == 'least_squares'):\n",
    "                            cost = self.least_squares_cost(y)\n",
    "                            print('Cost: ', cost)\n",
    "                elif(i%1000 == 0):\n",
    "                        if(cost_func == 'log_loss'):\n",
    "                            cost = self.log_loss(y)\n",
    "                            print('Cost: ', cost)\n",
    "                        elif(cost_func == 'least_squares'):\n",
    "                            cost = self.least_squares_cost(y)\n",
    "                            print('Cost: ', cost)\n",
    "                \n",
    "    def think(self,X):\n",
    "        activations = self.activations\n",
    "        a = [X] # Keep Track of activations\n",
    "        z = []\n",
    "        \n",
    "        # Add Bias\n",
    "        c = np.ones([1,a[0].shape[0]]).reshape(a[0].shape[0],1)\n",
    "#         print(a[0].shape)\n",
    "        a[0] = np.concatenate((c,a[0]), axis=1)\n",
    "        \n",
    "        for i in range(self.num_layers-1):\n",
    "            z.append(np.dot(a[i],self.weights[i]))\n",
    "            if(activations[i] == 'sigmoid'):\n",
    "                a.append(self.sigmoid(z[i]))\n",
    "            elif(activations[i] == 'reLU'):\n",
    "                a.append(self.reLU(z[i]))\n",
    "            elif(activations[i] == 'tanh'):\n",
    "                a.append(self.tanh(z[i]))\n",
    "            elif(activations[i] == 'softmax'):\n",
    "                a.append(self.softmax(z[i]))\n",
    "            elif(activations[i] == 'identity'):\n",
    "                a.append(self.identity(z[i]))\n",
    "        return a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = DeepNN([1,10,10,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-926642f25642>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tanh'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'identity'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'identity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'least_squares'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "NN.learn(2000,1e-5,X[:,1:],y,['tanh','identity','identity'], 'least_squares')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-773de79d3e27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "NN.think(X[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
